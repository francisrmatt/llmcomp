2024-09-14 14:14:23,992 root [INFO] ---- BEGINNING TRAINING ----
2024-09-14 14:14:23,994 root [INFO] New parameters
2024-09-14 14:14:23,994 root [INFO] Training with an extra 100000 steps with batch size 50
2024-09-14 14:14:23,994 root [INFO] Parameters for transformer are config=TransformerConfig(vocab_size=128, embedding_dim=128, num_layers=16, num_heads=8, emb_init_scale=0.02, widening_factor=4)
2024-09-14 14:14:23,996 get_data [INFO] Fetching data in chunk mode with all chunks each size 256
2024-09-14 14:14:23,996 get_data [INFO] Loading all data
2024-09-14 14:14:36,403 btransformer.train [INFO] Fetching random batch for fresh run
2024-09-14 14:14:38.468996: W external/xla/xla/service/gpu/nvptx_compiler.cc:836] The NVIDIA driver's CUDA version is 12.2 which is older than the PTX compiler version (12.6.20). Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.
2024-09-14 14:14:43,611 btransformer.train [INFO] Initialization done, starting training...
2024-09-14 14:15:08,391 btransformer.train [INFO] Step 0.000000, Loss 1320.372437, Grad norm 8.002284
2024-09-14 14:26:49,862 btransformer.train [INFO] Step 10000.000000, Loss 1066.920410, Grad norm 1.673703
2024-09-14 14:38:19,513 btransformer.train [INFO] Step 20000.000000, Loss 1004.396851, Grad norm 2.329493
2024-09-14 14:49:48,367 btransformer.train [INFO] Step 30000.000000, Loss 986.738403, Grad norm 3.807964
2024-09-14 15:01:18,181 btransformer.train [INFO] Step 40000.000000, Loss 888.434143, Grad norm 8.651359
2024-09-14 15:12:56,927 btransformer.train [INFO] Step 50000.000000, Loss 815.443787, Grad norm 3.189921
2024-09-14 15:24:42,086 btransformer.train [INFO] Step 60000.000000, Loss 766.130127, Grad norm 7.834634
2024-09-14 15:36:26,602 btransformer.train [INFO] Step 70000.000000, Loss 828.555786, Grad norm 3.741655
2024-09-14 15:48:11,338 btransformer.train [INFO] Step 80000.000000, Loss 776.998718, Grad norm 5.358601
2024-09-14 15:59:50,053 btransformer.train [INFO] Step 90000.000000, Loss 751.700500, Grad norm 5.828608
2024-09-14 16:11:19,611 root [INFO] 100000 training run complete (total 100000) in 7015.615660664975 seconds with loss 763.8912353515625
2024-09-14 16:11:19,693 root [INFO] Saved params in params/c256_010/params.npz file
n_files=100
2024-09-14 16:11:24,940 root [INFO] ---- BEGINNING TRAINING ----
2024-09-14 16:11:24,941 root [INFO] New parameters
2024-09-14 16:11:24,941 root [INFO] Training with an extra 100000 steps with batch size 50
2024-09-14 16:11:24,942 root [INFO] Parameters for transformer are config=TransformerConfig(vocab_size=128, embedding_dim=64, num_layers=16, num_heads=8, emb_init_scale=0.02, widening_factor=8)
2024-09-14 16:11:24,943 get_data [INFO] Fetching data in chunk mode with all chunks each size 256
2024-09-14 16:11:24,943 get_data [INFO] Loading all data
2024-09-14 16:11:37,261 btransformer.train [INFO] Fetching random batch for fresh run
2024-09-14 16:11:39.222829: W external/xla/xla/service/gpu/nvptx_compiler.cc:836] The NVIDIA driver's CUDA version is 12.2 which is older than the PTX compiler version (12.6.20). Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.
2024-09-14 16:11:46,385 btransformer.train [INFO] Initialization done, starting training...
2024-09-14 16:12:10,111 btransformer.train [INFO] Step 0.000000, Loss 1355.877197, Grad norm 6.365769
2024-09-14 16:21:07,087 btransformer.train [INFO] Step 10000.000000, Loss 1077.374390, Grad norm 0.796225
2024-09-14 16:29:46,818 btransformer.train [INFO] Step 20000.000000, Loss 1063.385498, Grad norm 1.661490
2024-09-14 16:38:12,005 btransformer.train [INFO] Step 30000.000000, Loss 1046.770630, Grad norm 1.912807
2024-09-14 16:46:36,885 btransformer.train [INFO] Step 40000.000000, Loss 1006.929871, Grad norm 5.224664
2024-09-14 16:55:01,731 btransformer.train [INFO] Step 50000.000000, Loss 1031.324707, Grad norm 3.211064
2024-09-14 17:03:26,088 btransformer.train [INFO] Step 60000.000000, Loss 979.695435, Grad norm 2.716763
2024-09-14 17:11:51,245 btransformer.train [INFO] Step 70000.000000, Loss 944.889832, Grad norm 7.469386
2024-09-14 17:20:16,207 btransformer.train [INFO] Step 80000.000000, Loss 870.062012, Grad norm 8.367763
2024-09-14 17:28:41,555 btransformer.train [INFO] Step 90000.000000, Loss 930.440063, Grad norm 4.284111
2024-09-14 17:37:07,914 root [INFO] 100000 training run complete (total 100000) in 5142.971223991015 seconds with loss 882.0899047851562
2024-09-14 17:37:07,966 root [INFO] Saved params in params/c256_011/params.npz file
n_files=100
2024-09-14 17:37:13,099 root [INFO] ---- BEGINNING TRAINING ----
2024-09-14 17:37:13,101 root [INFO] New parameters
2024-09-14 17:37:13,101 root [INFO] Training with an extra 100000 steps with batch size 50
2024-09-14 17:37:13,101 root [INFO] Parameters for transformer are config=TransformerConfig(vocab_size=128, embedding_dim=32, num_layers=16, num_heads=8, emb_init_scale=0.02, widening_factor=16)
2024-09-14 17:37:13,102 get_data [INFO] Fetching data in chunk mode with all chunks each size 256
2024-09-14 17:37:13,102 get_data [INFO] Loading all data
2024-09-14 17:37:25,415 btransformer.train [INFO] Fetching random batch for fresh run
2024-09-14 17:37:27.346189: W external/xla/xla/service/gpu/nvptx_compiler.cc:836] The NVIDIA driver's CUDA version is 12.2 which is older than the PTX compiler version (12.6.20). Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.
2024-09-14 17:37:34,890 btransformer.train [INFO] Initialization done, starting training...
2024-09-14 17:37:58,361 btransformer.train [INFO] Step 0.000000, Loss 1345.389038, Grad norm 4.448105
2024-09-14 17:45:38,995 btransformer.train [INFO] Step 10000.000000, Loss 1089.660278, Grad norm 0.569255
2024-09-14 17:53:15,819 btransformer.train [INFO] Step 20000.000000, Loss 1078.471069, Grad norm 0.572560
2024-09-14 18:00:54,495 btransformer.train [INFO] Step 30000.000000, Loss 1070.293701, Grad norm 0.784877
2024-09-14 18:08:32,250 btransformer.train [INFO] Step 40000.000000, Loss 1074.224243, Grad norm 1.252105
2024-09-14 18:16:09,659 btransformer.train [INFO] Step 50000.000000, Loss 1072.754517, Grad norm 1.134935
2024-09-14 18:23:49,533 btransformer.train [INFO] Step 60000.000000, Loss 1057.970337, Grad norm 3.352347
2024-09-14 18:31:44,199 btransformer.train [INFO] Step 70000.000000, Loss 1054.223999, Grad norm 3.892313
2024-09-14 18:39:21,200 btransformer.train [INFO] Step 80000.000000, Loss 1024.649536, Grad norm 2.853088
2024-09-14 18:46:54,907 btransformer.train [INFO] Step 90000.000000, Loss 981.520081, Grad norm 6.339045
2024-09-14 18:54:33,040 root [INFO] 100000 training run complete (total 100000) in 4639.936946475005 seconds with loss 952.9666748046875
2024-09-14 18:54:33,090 root [INFO] Saved params in params/c256_012/params.npz file
n_files=100
2024-09-14 18:54:38,486 root [INFO] ---- BEGINNING TRAINING ----
2024-09-14 18:54:38,487 root [INFO] New parameters
2024-09-14 18:54:38,487 root [INFO] Training with an extra 100000 steps with batch size 50
2024-09-14 18:54:38,487 root [INFO] Parameters for transformer are config=TransformerConfig(vocab_size=128, embedding_dim=16, num_layers=16, num_heads=8, emb_init_scale=0.02, widening_factor=16)
2024-09-14 18:54:38,489 get_data [INFO] Fetching data in chunk mode with all chunks each size 256
2024-09-14 18:54:38,489 get_data [INFO] Loading all data
2024-09-14 18:54:50,810 btransformer.train [INFO] Fetching random batch for fresh run
2024-09-14 18:54:52.712855: W external/xla/xla/service/gpu/nvptx_compiler.cc:836] The NVIDIA driver's CUDA version is 12.2 which is older than the PTX compiler version (12.6.20). Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.
2024-09-14 18:55:00,762 btransformer.train [INFO] Initialization done, starting training...
2024-09-14 18:55:23,598 btransformer.train [INFO] Step 0.000000, Loss 1307.377197, Grad norm 2.839497
2024-09-14 19:02:08,468 btransformer.train [INFO] Step 10000.000000, Loss 1102.083740, Grad norm 0.460032
2024-09-14 19:08:49,358 btransformer.train [INFO] Step 20000.000000, Loss 1082.980225, Grad norm 0.538236
2024-09-14 19:15:30,351 btransformer.train [INFO] Step 30000.000000, Loss 1083.613892, Grad norm 0.510908
2024-09-14 19:22:09,702 btransformer.train [INFO] Step 40000.000000, Loss 1082.328613, Grad norm 1.011153
2024-09-14 19:28:42,935 btransformer.train [INFO] Step 50000.000000, Loss 1083.460815, Grad norm 0.734769
2024-09-14 19:35:21,066 btransformer.train [INFO] Step 60000.000000, Loss 1077.990601, Grad norm 1.676242
2024-09-14 19:42:01,838 btransformer.train [INFO] Step 70000.000000, Loss 1077.532959, Grad norm 0.955738
2024-09-14 19:48:42,810 btransformer.train [INFO] Step 80000.000000, Loss 1073.052002, Grad norm 1.215870
2024-09-14 19:55:23,757 btransformer.train [INFO] Step 90000.000000, Loss 1068.084229, Grad norm 0.976436
2024-09-14 20:02:05,946 root [INFO] 100000 training run complete (total 100000) in 4047.45675451 seconds with loss 1071.1832275390625
2024-09-14 20:02:05,987 root [INFO] Saved params in params/c256_013/params.npz file
n_files=100
2024-09-14 20:02:11,418 root [INFO] ---- BEGINNING TRAINING ----
2024-09-14 20:02:11,419 root [INFO] New parameters
2024-09-14 20:02:11,419 root [INFO] Training with an extra 100000 steps with batch size 50
2024-09-14 20:02:11,419 root [INFO] Parameters for transformer are config=TransformerConfig(vocab_size=128, embedding_dim=8, num_layers=16, num_heads=8, emb_init_scale=0.02, widening_factor=16)
2024-09-14 20:02:11,420 get_data [INFO] Fetching data in chunk mode with all chunks each size 256
2024-09-14 20:02:11,420 get_data [INFO] Loading all data
2024-09-14 20:02:23,724 btransformer.train [INFO] Fetching random batch for fresh run
2024-09-14 20:02:25.555644: W external/xla/xla/service/gpu/nvptx_compiler.cc:836] The NVIDIA driver's CUDA version is 12.2 which is older than the PTX compiler version (12.6.20). Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.
2024-09-14 20:02:30,808 btransformer.train [INFO] Initialization done, starting training...
2024-09-14 20:02:51,173 btransformer.train [INFO] Step 0.000000, Loss 1369.121338, Grad norm 1.381312
2024-09-14 20:05:40,197 btransformer.train [INFO] Step 10000.000000, Loss 1147.183838, Grad norm 0.237928
2024-09-14 20:08:24,748 btransformer.train [INFO] Step 20000.000000, Loss 1109.908203, Grad norm 0.400842
2024-09-14 20:11:06,686 btransformer.train [INFO] Step 30000.000000, Loss 1092.175903, Grad norm 0.262284
2024-09-14 20:13:48,648 btransformer.train [INFO] Step 40000.000000, Loss 1088.912842, Grad norm 0.353904
2024-09-14 20:16:30,750 btransformer.train [INFO] Step 50000.000000, Loss 1091.630493, Grad norm 0.483160
2024-09-14 20:19:12,817 btransformer.train [INFO] Step 60000.000000, Loss 1092.769165, Grad norm 0.494378
2024-09-14 20:21:52,857 btransformer.train [INFO] Step 70000.000000, Loss 1087.991089, Grad norm 0.403622
2024-09-14 20:24:34,397 btransformer.train [INFO] Step 80000.000000, Loss 1089.603394, Grad norm 0.355044
2024-09-14 20:27:16,429 btransformer.train [INFO] Step 90000.000000, Loss 1089.697754, Grad norm 0.386631
2024-09-14 20:29:59,827 root [INFO] 100000 training run complete (total 100000) in 1668.4065133570111 seconds with loss 1084.46630859375
2024-09-14 20:29:59,867 root [INFO] Saved params in params/c256_014/params.npz file
n_files=100
